import numpyro
import numpyro.distributions as dist
import pandas as pd
import jax
import jax.numpy as jnp
from jax import random
import numpy as np
from numpyro.infer import NUTS, MCMC
from numpyro.infer.reparam import LocScaleReparam
import numpyro.distributions.transforms as T
from pathlib import Path
import matplotlib.pyplot as plt

numpyro.set_platform("cpu") 

def irt_2pl(resp, N, K, beta=1, **hyper_params):
   
    hyper_a_sd = hyper_params.get('a', 0.35)
    hyper_b_sd = hyper_params.get('b', 1.0)
        
    theta1 = numpyro.sample("theta1", dist.Normal(0, 1).expand([N]).to_event(1))
    theta = numpyro.deterministic("theta", theta1 - jnp.mean(theta1, axis=0, keepdims=True))

    log_a_sigma = numpyro.sample("log_a_sigma", dist.HalfNormal(hyper_a_sd))
    a = numpyro.sample("a",
                       dist.TruncatedNormal(1., log_a_sigma, low=0).expand([K]).to_event(1)
    )

    b_sd = numpyro.sample("b_sd", dist.HalfNormal(hyper_b_sd))
    with numpyro.handlers.reparam(config={"b": LocScaleReparam()}):
        b = numpyro.sample("b", dist.Normal(0., b_sd).expand([K]).to_event(1))
  
    logit = (a * (theta[:, None] - b))
    p = jax.nn.sigmoid(logit)

    with numpyro.handlers.scale(scale=beta):
        with numpyro.plate("person", N):
            numpyro.sample(
                "obs",
                dist.Bernoulli(probs=p).to_event(1),
                obs=resp
            )


def main():
    """メイン処理"""
    
    # ------------------------------ 設定
    # データファイルのパス
    data_file = r"C:\Users\smatsuzono\Documents\ZETEC\Python\Agent\JSNDI_WORKBOOK_RAG_v3_20251123.xlsx"
    
    # モデル名の定義
    models = [
        'gpt-5-mini', 'gemini-2.5-flash', 'claude-sonnet-4', 'GPT-o4-mini', 'claude-3-5-haiku', 'gpt-4.1', 'gemini-3-pro',
        'RAG-gpt-5-mini', 'RAG-gemini-2.5-flash', 'RAG-claude-sonnet-4', 'RAG-GPT-o4-mini', 'RAG-claude-3-5-haiku', 'RAG-gpt-4.1', 'RAG-gemini-3-pro'
    ]
    
    # 解析対象の科目
    subject = 'JIS'
    
    # 解析方法の選択
    analysis_method = 'pattern_combined'  # 'pattern_combined' or 'pattern_separate'
    
    # MCMC設定
    num_warmup = 2_000
    num_samples = 2_000
    num_chains = 4
    
    # Chain設定
    avail_devices = jax.local_device_count()
    CHAIN_METHOD = "parallel" if avail_devices >= num_chains else "vectorized"
    print(f"[NumPyro] devices={avail_devices}, num_chains={num_chains}, chain_method={CHAIN_METHOD}")
    
    # ------------------------ データ読み込み
    print("データを読み込んでいます...")
    try:
        df = pd.read_excel(data_file)
        print(f"データ読み込み完了: {df.shape[0]}行 x {df.shape[1]}列")
    except FileNotFoundError:
        print(f"エラー: ファイルが見つかりません: {data_file}")
        print("ファイルパスを確認してください。")
        return
    except Exception as e:
        print(f"データ読み込みエラー: {e}")
        return
    
    # --------------------------データ前処理
    print("\nデータを前処理しています...")
    print(f"利用可能な列: {list(df.columns)}")
    
    # カテゴリ列のリスト
    category_columns = [
        'MTA_WELD', 'JIS', 'Code_Know', 'TBEx_Know', 'TBEx_Calc',
    ]
    
    # 利用可能なカテゴリを確認
    available_categories = [col for col in category_columns if col in df.columns]
    print(f"利用可能なカテゴリ: {available_categories}")
    
    # 指定されたカテゴリが存在するか確認
    if subject not in df.columns:
        print(f"エラー: カテゴリ '{subject}' が見つかりません")
        print(f"利用可能なカテゴリ: {available_categories}")
        return
    
    # 指定カテゴリのデータをフィルタリング（該当カテゴリが1の行を抽出）
    category_data = df[df[subject] == 1].copy()
    
    if category_data.empty:
        print(f"警告: カテゴリ '{subject}' に該当するデータがありません")
        return
    
    print(f"カテゴリ '{subject}' のデータ数: {len(category_data)}行")
    print(f"一意な問題数: {category_data['file'].nunique()}")
    print(f"Patternの種類: {category_data['Pattern'].unique()}")
    
    # 解析方法に応じたデータ整形
    if analysis_method == 'pattern_combined':
        print("\n【Pattern統合モード】各Patternの平均正答率を使用します")

        # 各モデル・各問題の平均正答率を計算（和集合で列をそろえる）
        response_by_model = {}
        all_files = set()

        for model in models:
            if model not in df.columns:
                print(f"警告: モデル列 '{model}' が見つかりません")
                continue

            model_data = category_data[category_data[model] == 1]
            if model_data.empty:
                print(f"警告: モデル '{model}' のデータがありません")
                continue

            file_scores = model_data.groupby('file')['Correct'].mean()  # Series(index=file)
            response_by_model[model] = file_scores
            all_files |= set(file_scores.index)
            print(f"モデル '{model}': {len(model_data)}回答 → {len(file_scores)}問題")

        if not response_by_model:
            print("エラー: 解析可能なデータがありません")
            return

        all_files = sorted(all_files)
        response_matrix = []
        model_names = []
        for model, s in response_by_model.items():
            response_matrix.append(s.reindex(all_files).to_numpy())
            model_names.append(model)

        response = pd.DataFrame(response_matrix, index=model_names, columns=all_files)
    
    else:  # pattern_separate
        print("\n【Pattern分離モード】各Pattern×fileの組み合わせを別項目として扱います")
        
        # file_patternという新しい識別子を作成
        category_data['file_pattern'] = category_data['file'] + '_P' + category_data['Pattern'].astype(str)
        unique_items = category_data['file_pattern'].unique()
        
        response_matrix = []
        model_names = []
        
        for model in models:
            if model not in df.columns:
                continue
            
            model_data = category_data[category_data[model] == 1]
            if model_data.empty:
                continue
            
            # 各file_patternに対する応答を取得
            model_responses = []
            for item in unique_items:
                item_data = model_data[model_data['file_pattern'] == item]
                if not item_data.empty:
                    model_responses.append(item_data['Correct'].iloc[0])
                else:
                    model_responses.append(np.nan)
            
            response_matrix.append(model_responses)
            model_names.append(model)
        
        response = pd.DataFrame(response_matrix, index=model_names, columns=unique_items)
    
    
    # 欠損値の処理
    if response.isnull().any().any():
        print(f"\n警告: 欠損値が検出されました")
        print(f"欠損値の数: {response.isnull().sum().sum()}")
        # 欠損値を含む列を削除
        response = response.dropna(axis=1)
        print(f"欠損値を含む項目を削除後: {response.shape}")
    
    print(f"\n最終的な応答行列の形状: {response.shape}")
    print(f"モデル数 (N): {response.shape[0]}, 項目数 (K): {response.shape[1]}")
    print(f"解析対象モデル: {list(response.index)}")
    
    # --------------- MCMCサンプリング
    print("\nMCMCサンプリングを開始します...")

    # データの次元
    N, K = response.shape

    if N < 2 or K < 2:
        print(f"エラー: データが不十分です（モデル数: {N}, 項目数: {K}）")
        print("少なくとも2つ以上のモデルと2つ以上の項目が必要です")
        return

    master_key = random.PRNGKey(0)
    nuts_kernel = NUTS(irt_2pl, target_accept_prob=0.9)
    mcmc = MCMC(
        nuts_kernel,
        num_warmup=num_warmup,
        num_samples=num_samples,
        num_chains=num_chains,
        chain_method=CHAIN_METHOD,
        progress_bar=True,
    )

    try:
        mcmc.run(master_key, resp=response.values, N=N, K=K, beta=1)
        print("MCMCサンプリング完了")
    except Exception as e:
        print(f"MCMCサンプリングエラー: {e}")
        return

        import numpyro.diagnostics as diag
    samples_by_chain = mcmc.get_samples(group_by_chain=True)  # (chains, draws, ...)

    
    # ---------------------------------------------- 結果の取得と保存
    print("\n結果を保存しています...")
    
    # サンプルの取得
    a = mcmc.get_samples()['a']
    b = mcmc.get_samples()['b']
    theta = mcmc.get_samples()['theta']
    
    print(f"パラメータ a の形状: {a.shape}")
    print(f"パラメータ b の形状: {b.shape}")
    print(f"パラメータ theta の形状: {theta.shape}")
    
    # 結果の保存
    result_path = Path("resultsRAGCOMB")
    result_path = result_path / subject / analysis_method
    result_path.mkdir(parents=True, exist_ok=True)
    
    # パラメータをDataFrameとして整理
    a_df = pd.DataFrame(a, columns=response.columns)
    b_df = pd.DataFrame(b, columns=response.columns)
    theta_df = pd.DataFrame(theta, columns=response.index)
    
    # CSVファイルとして保存
    a_df.to_csv(result_path / 'a.csv', index=False)
    print(f"保存完了: {result_path / 'a.csv'}")
    
    b_df.to_csv(result_path / 'b.csv', index=False)
    print(f"保存完了: {result_path / 'b.csv'}")
    
    theta_df.to_csv(result_path / 'theta.csv', index=False)
    print(f"保存完了: {result_path / 'theta.csv'}")
    
    # 項目情報も保存
    item_info = pd.DataFrame({
        'item': response.columns,
        'a_mean': np.mean(a, axis=0),
        'a_std': np.std(a, axis=0),
        'b_mean': np.mean(b, axis=0),
        'b_std': np.std(b, axis=0)
    })
    item_info.to_csv(result_path / 'item_statistics.csv', index=False)
    
    # モデル情報も保存
    model_info = pd.DataFrame({
        'model': response.index,
        'theta_mean': np.mean(theta, axis=0),
        'theta_std': np.std(theta, axis=0)
    })
    model_info.to_csv(result_path / 'model_statistics.csv', index=False)
    
    # ------------------------------- ICC（Item Characteristic Curve）をシグモイドで描く

    # 事後平均
    a_mean = np.asarray(a).mean(axis=0)      # (K,)
    b_mean = np.asarray(b).mean(axis=0)      # (K,)
    theta_mean = np.asarray(theta).mean(axis=0)  # (N,)

    # 出力先
    fig_dir = result_path / "figs_icc"
    fig_dir.mkdir(parents=True, exist_ok=True)

    # θ性的描画レンジ
    theta_grid = np.linspace(-5, 5, 301)
    
    # θ動的描画レンジ
    # lo = min(theta_mu.min() - 3*theta_sd.max(), -3.0)
    # hi = max(theta_mu.max() + 3*theta_sd.max(),  3.0)
    # theta_grid = np.linspace(lo, hi, 301)

    def icc_curve(theta_grid, a_val, b_val):
        # 2PL: sigmoid(a*(theta - b))
        return 1.0 / (1.0 + np.exp(-a_val * (theta_grid - b_val)))

    # --- 共通：各LLMの θ を縦線で重ねる ---
    def add_theta_vlines(ax, model_names, theta_mean):
        for name, mu in zip(model_names, theta_mean):
            ax.axvline(mu, color="C7", linestyle="--", alpha=0.35)
            ax.text(mu, 0.02, name, rotation=90, va="bottom", ha="right", fontsize=8, alpha=0.7)

    # 識別力上位N本をまとめ描き
    topN = 6
    idx_top = np.argsort(-a_mean)[:topN]
    plt.figure(figsize=(7, 5))
    for i in idx_top:
        p = icc_curve(theta_grid, a_mean[i], b_mean[i])
        label = f"{response.columns[i]} | a={a_mean[i]:.2f}, b={b_mean[i]:.2f}"
        plt.plot(theta_grid, p, label=label)
    add_theta_vlines(plt.gca(), list(response.index), theta_mean)
    plt.xlabel("Ability θ"); plt.ylabel("P(correct)")
    plt.title(f"ICC (Top-{topN} by discrimination a)")
    plt.ylim(0, 1); plt.legend(fontsize=8)
    plt.tight_layout()
    plt.savefig(fig_dir / f"icc_top{topN}.png", dpi=150)
    plt.close()

    print(f"ICC図を保存しました → {fig_dir}")


    # カテゴリ全体をシグモイドで可視化する
    fig_dir = (result_path / "figs_category").resolve()
    fig_dir.mkdir(parents=True, exist_ok=True)

    # 入力（A: 配列がある場合 / B: 要約値だけで描く場合）
    have_arrays = True

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    # a,b の「項目ごとの事後平均」をまず出す
    a_arr = np.asarray(a)      # shape: (draws, K)
    b_arr = np.asarray(b)      # shape: (draws, K)
    theta_arr = np.asarray(theta)  # shape: (draws, N)

    a_mean_items = a_arr.mean(axis=0)              # (K,)
    b_mean_items = b_arr.mean(axis=0)              # (K,)

    # カテゴリ代表値（平均・標準偏差）を都度集計
    cat_a_mean = float(a_mean_items.mean())
    cat_a_sd   = float(a_mean_items.std(ddof=0))
    cat_b_mean = float(b_mean_items.mean())
    cat_b_sd   = float(b_mean_items.std(ddof=0))

    # モデルごとの θ の平均・標準偏差を都度集計（名前は response.index から）
    theta_mu = theta_arr.mean(axis=0)              # (N,)
    theta_sd = theta_arr.std(axis=0, ddof=0)       # (N,)
    theta_stats = {
        name: (float(mu), float(sd))
        for name, mu, sd in zip(list(response.index), theta_mu, theta_sd)
    }

    # 固定描画レンジ
    theta_grid = np.linspace(-5, 5, 301)
    
    # 動的描画レンジ
    # lo = min(theta_mu.min() - 3*theta_sd.max(), -3.0)
    # hi = max(theta_mu.max() + 3*theta_sd.max(),  3.0)
    # theta_grid = np.linspace(lo, hi, 301)

    # 1) 代表値ICC： sigmoid(ā(θ−b̄))
    if have_arrays:
        # 項目ごとの事後平均
        a_mean_items = np.asarray(a).mean(axis=0)  # shape: (K,)
        b_mean_items = np.asarray(b).mean(axis=0)  # shape: (K,)
        a_bar = float(a_mean_items.mean())
        b_bar = float(b_mean_items.mean())
    else:
        a_bar = cat_a_mean
        b_bar = cat_b_mean

    p_rep = sigmoid(a_bar * (theta_grid - b_bar))

    plt.figure(figsize=(7,5))
    plt.plot(theta_grid, p_rep, label=f"Representative ICC (ā={a_bar:.2f}, b̄={b_bar:.2f})")
    plt.ylim(0,1); plt.xlabel("Ability θ"); plt.ylabel("P(correct)")
    plt.title("Category-level ICC (Representative)")
    # 各モデルのθ平均を縦線で
    for name, (mu, sd) in theta_stats.items():
        plt.axvline(mu, linestyle="--", alpha=0.4)
        plt.text(mu, 0.02, name, rotation=90, va="bottom", ha="right", fontsize=8)
    plt.tight_layout()
    plt.savefig(fig_dir / "category_icc_representative.png", dpi=150)
    plt.close()

    # 2) 集計ICC：平均正答率 \bar{p}(θ) = 平均_k sigmoid(a_k(θ-b_k))
    # 2-1) 項目ごとの事後平均だけで描く（準厳密）
    if have_arrays:
        a_mean_items = np.asarray(a).mean(axis=0)  # (K,)
        b_mean_items = np.asarray(b).mean(axis=0)  # (K,)
        p_items = sigmoid(np.outer(theta_grid, a_mean_items) - np.outer(theta_grid*0 + 1, a_mean_items*b_mean_items))
        p_mean  = p_items.mean(axis=1)  # 各θで項目平均
    else:
        # 要約値のみしか無い場合の簡易近似（a,bを正規近似から乱数生成し a>0 をクリップ）
        rng = np.random.default_rng(0)
        S = 4000
        a_draws = rng.normal(cat_a_mean, cat_a_sd, size=S)
        a_draws = np.clip(a_draws, 1e-6, None)  # a>0 に近似的に制約
        b_draws = rng.normal(cat_b_mean, cat_b_sd, size=S)
        # カテゴリ内に「仮想的な項目が多数ある」とみなして集計ICCを近似
        p_s = sigmoid(np.outer(theta_grid, a_draws) - np.outer(theta_grid*0 + 1, a_draws*b_draws))  # (G,S)
        p_mean = p_s.mean(axis=1)

    plt.figure(figsize=(7,5))
    plt.plot(theta_grid, p_mean, label="Aggregate ICC (mean over items)")
    plt.ylim(0,1); plt.xlabel("Ability θ"); plt.ylabel("E[P(correct)] across items")
    plt.title("Category-level Aggregate ICC")
    for name, (mu, sd) in theta_stats.items():
        plt.axvline(mu, linestyle="--", alpha=0.3)
        plt.text(mu, 0.02, name, rotation=90, va="bottom", ha="right", fontsize=8)
    plt.tight_layout()
    plt.savefig(fig_dir / "category_icc_aggregate_mean.png", dpi=150)
    plt.close()

    # 2-2) 事後不確実性の帯（a,b のMCMCサンプルがある場合）
    if have_arrays:
        # 各drawごとに、\bar{p}(θ) を計算し分位点で帯を作る
        # a, b: (draws, K)
        a_arr = np.asarray(a)
        b_arr = np.asarray(b)
        draws = a_arr.shape[0]
        # メモリ節約のため、間引き（任意）
        step = max(1, draws // 2000)
        a_sub = a_arr[::step]  # (~2000, K)
        b_sub = b_arr[::step]
        p_theta_draws = []
        for A, B in zip(a_sub, b_sub):
            # 各θで項目平均
            p_items = sigmoid(np.outer(theta_grid, A) - np.outer(theta_grid*0 + 1, A*B))
            p_theta_draws.append(p_items.mean(axis=1))  # (G,)
        p_theta_draws = np.stack(p_theta_draws, axis=0)  # (D, G)
        low = np.percentile(p_theta_draws, 2.5, axis=0)
        med = np.percentile(p_theta_draws, 50,  axis=0)
        high= np.percentile(p_theta_draws, 97.5,axis=0)

        plt.figure(figsize=(7,5))
        plt.plot(theta_grid, med, label="Aggregate ICC (median)")
        plt.fill_between(theta_grid, low, high, alpha=0.2, label="95% credible band")
        plt.ylim(0,1); plt.xlabel("Ability θ"); plt.ylabel("E[P(correct)] across items")
        plt.title("Category-level Aggregate ICC with 95% CrI")
        for name, (mu, sd) in theta_stats.items():
            plt.axvline(mu, linestyle="--", alpha=0.3)
            plt.text(mu, 0.02, name, rotation=90, va="bottom", ha="right", fontsize=8)
        plt.tight_layout()
        plt.savefig(fig_dir / "category_icc_aggregate_credible_band.png", dpi=150)
        plt.close()

    # 3) 各モデルθでのカテゴリ期待正答率を数値で出力
    def p_rep_at(mu_theta):
        return sigmoid(a_bar * (mu_theta - b_bar))

    def p_agg_at(mu_theta):
        if have_arrays:
            return float(sigmoid(a_mean_items * (mu_theta - b_mean_items)).mean())
        else:
            # 正規近似サンプルで近似
            rng = np.random.default_rng(1)
            S = 20000
            a_draws = rng.normal(cat_a_mean, cat_a_sd, size=S)
            a_draws = np.clip(a_draws, 1e-6, None)
            b_draws = rng.normal(cat_b_mean, cat_b_sd, size=S)
            return float(sigmoid(a_draws * (mu_theta - b_draws)).mean())

    print("\n=== Category-level expected P(correct) at model θ (representative / aggregate) ===")
    for name, (mu, sd) in theta_stats.items():
        print(f"{name:16s}: rep={p_rep_at(mu):.3f} | agg={p_agg_at(mu):.3f}")

    
if __name__ == "__main__":
    main()
