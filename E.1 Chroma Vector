def split_docs(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=150,
        separators=["\n\n", "\n", "。", "、", " ", ""],
        keep_separator=False
    )
    chunks = splitter.split_documents(docs)
    
    for i, d in enumerate(chunks):
        d.metadata["chunk_index"] = i
    return chunks

def deterministic_id(text: str, meta: dict) -> str:
    base = f"{meta.get('source')}|{meta.get('page')}|{meta.get('chunk_index')}|{text}".encode("utf-8")
    return hashlib.sha256(base).hexdigest()

def ingest_chunked(chunks, vector_store):
    existing_ids = get_all_ids(vector_store)
    new_docs, new_ids = [], []
    
    for d in chunks:
        did = deterministic_id(d.page_content, d.metadata)
        if did not in existing_ids:
            new_docs.append(d)
            new_ids.append(did)
            
    if new_docs:
        vector_store.add_documents(new_docs, ids=new_ids)
    return len(new_docs)
